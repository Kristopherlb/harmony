Harmony UX Strategy Report
Competitive UX Analysis & Trends (2024–2025)

Shift to Infinite Canvas for Workflows: Modern automation tools are moving from rigid step-by-step editors to flexible “infinite canvas” interfaces. Platforms like Make (formerly Integromat) and n8n have long used node-based canvases for visually arranging workflow steps. Recently, Zapier introduced Canvas, a free-form planning space where users map out processes with AI assistance. These canvases let users pan/zoom and organize complex workflows non-linearly, mirroring the way architects or designers map systems. An infinite canvas helps users see branching logic and parallel paths at a glance, which is increasingly important for multi-step AI-driven workflows.

AI-Assisted Workflow Building: A major trend is embedding AI to help configure and even generate workflow logic. Zapier Copilot and Zapier AI features, for example, let users describe an automation in plain language and get a draft Zap built for them. In the n8n community, there’s active interest in an LLM that creates workflows from descriptions (“pull tweets, analyze sentiment, email summary”). Canvas by Thesys goes further: it not only provides a canvas but also an AI design copilot that can turn natural language prompts or even hand-drawn sketches into configured workflow nodes automatically. This pattern of “describe what you need, then tweak the generated workflow” is emerging as a 2025 norm, lowering the barrier for non-technical users.

Unified Prompt Engineering vs. Logic Flows: There’s a divergence in approach between AI-first platforms and traditional logic-oriented ones. Vellum exemplifies the AI-native approach – it focuses on prompt engineering and agent behaviors, allowing users to “describe what you want done with no code, no workflow wiring required”. You might simply specify an outcome (“When an incident is detected, have an agent diagnose it”) and Vellum’s AI generates the underlying steps. In contrast, Make (and Zapier historically) require explicit logic wiring – users connect triggers to actions, map data fields, and insert filters manually. Vellum’s flow is akin to teaching an AI agent, whereas Make’s flow is like programming a process diagram. For an agentic Ops platform like Harmony – which must handle dynamic, AI-driven decisions – Vellum’s declarative, AI-driven UX offers agility. It aligns with letting an AI orchestrator figure out the steps. However, Make’s explicit logic UI offers transparency and control that Ops engineers expect for reliability. A hybrid approach may be ideal: use natural language to draft workflows, then allow visual refinement – leveraging Vellum’s ease without losing Make’s clarity.

Real-time Workflow Monitoring & Visualization: Long-running processes demand new visualization patterns. Temporal (a code-first workflow engine) addressed this with a timeline view to portray workflow executions over time. Instead of a static flowchart, Temporal’s UI shows a scrollable timeline of events (activities started/completed, timers, retries) with colored bars indicating duration and status. This design acknowledges that in complex Ops scenarios, runtime behavior (parallel tasks, delays, retries) can’t be fully conveyed by a static graph. The timeline view is a trend toward blending monitoring into the UX – users can literally see an incident workflow’s progress and parallel branches in real time. Harmony should emulate this by presenting both the logical structure and the live state (e.g. a running incident response playbook visualized as a timeline or Gantt-like chart).

Bridging Code and UI (Preserving Fidelity): A challenge in 2025’s tools is reconciling low-code UX with the power of code. Graphical workflow editors often break down when logic gets complex – as Temporal’s founder notes, “a mutable, dynamic graph is something that virtually no graph-based language supports well,” especially for AI agents deciding steps at runtime. Many platforms (Workato, Tray.io, etc.) allow code steps or custom scripts as escape hatches. Temporal’s philosophy is that workflow-as-code yields maximum fidelity (loops, conditionals, error handling are all just code). The trend is to offer a dual representation: use code for the heavy logic, but visualize the high-level structure for usability. For example, some Temporal users parse their TypeScript definitions to generate a high-level diagram, and Temporal’s UI shows state and history but not every code detail. Harmony should strive for a code-UI bridge: perhaps a view where each visual node corresponds to a snippet of real code, ensuring what you see is what the engine runs. The key is to avoid “toy” UIs that oversimplify – users must be able to drill down into code when needed, while still benefiting from a visual overview.

Other Notable UX Patterns: Many platforms are adding collaborative and version-controlled UX features. Workato and Harness, catering to enterprises, allow templatizing and reusing workflow pieces (Harness has a Template Library for steps, stages, pipelines to promote reuse across teams). We also see integrated testing/simulation modes (e.g. stepping through a Zap with sample data, or Vellum’s prompt sandbox). And given the rise of agents, some tools (Zapier’s new Agents beta, etc.) include chat-style interfaces to interact with automations in real time. In summary, the UX trend is convergence: free-form visual builders, AI-assisted development, real-time monitoring, and code-level power all coming together.

Asset Discovery & Library UX
Patterns in App Marketplaces & Template Galleries

Zapier’s App Marketplace: Zapier sets the benchmark with over 8,000 integrations. Its App Integrations directory uses categories and filters to tame this vast list. Users can browse by category (“Communication”, “IT Operations”, “AI Agents”, etc.) and sub-category, or sort by popularity and newest. Each app is shown with an icon and name, and labels if it’s “Premium” or in “Beta”. This marketplace style helps users quickly find a tool (Capability) they need. Zapier also has a Template Gallery with pre-built workflow recipes: it’s organized by use case and curated collections (e.g. “HR onboarding templates”, “Marketing starter kit”). Notably, Zapier’s gallery features multi-step solutions (they call them “solutions” or “Zaps”) presented as cards with a short description. The best practice here is to offer multiple discovery pathways – by app, by use-case, by popularity – to accommodate how different users think about their problems.

Make (Integromat) and n8n: These platforms also have libraries of integrations and templates, though on a smaller scale. Make’s scenario builder provides a catalog of pre-built modules for various apps, searchable and categorized by function (e.g. “CRM”, “DevOps”). n8n, being open-source, fosters a community-driven collection of workflows. They present templates on their website and within the app, often tagged by function (like “Twitter to Slack notifier”). A common pattern is visual cards for each template showing the app logos involved – this immediately communicates what the integration is (e.g. a card showing the Slack and GitHub logos for a “Slack to GitHub Issue” workflow). Harness’s Template Library similarly presents reusable pipeline components with clear naming and scopes (account, org, project), though it’s more for internal reuse than a public marketplace.

Installation & Setup Flows: All these platforms emphasize a smooth “add to my workspace” experience. For example, when you choose a Zapier template, it launches a setup wizard: you’re prompted to connect the necessary accounts (OAuth flows pop up for Google, Slack, etc.), then map any required fields. The credential management UX is critical: Zapier and Make both have a centralized “My Apps” section where users can pre-authorize connections. Best practice is to onboard users contextually – if a user selects a Capability (integration) from Harmony’s library, immediately guide them through connecting their account or API key, with clear status indicators (e.g. a checkmark when “Slack is connected” and warnings if not). Providing in-app OAuth modals or token input fields, and showing the security scopes requested, builds trust during this process.

Search and Relationship Mapping: Users often know their end goal but not the exact tool or workflow they need. Platforms like Zapier handle this by allowing natural language search (“email to Slack notification”) which returns relevant templates. Another emerging practice is showing relationships between assets – for instance, Zapier’s template pages show which apps are involved, and clicking an app shows all templates involving it. Harmony can adopt this by letting users see, for example, all Blueprints that use a particular Capability or all Skills related to a given Blueprint. This cross-linking turns the library into a navigable graph of possibilities (much like browsing on Amazon: looking at a tool, you see “recipes using this tool”). Such relational browsing can inspire users to install additional Capabilities once they see what’s possible with them.

Recommendations for Harmony’s Library UX

Concept 1: Unified Marketplace with Smart Filtering – Harmony should provide a one-stop Library where Capabilities, Blueprints, and Skills are discoverable in one place, with the ability to filter by type and domain. For example, a user could search “Jira” and see the Jira Capability (integration), any Blueprints involving Jira (e.g. “Incident Postmortem with Jira” template), and relevant Skills (e.g. “Create Jira Ticket” agent skill). This unified view, inspired by Zapier’s approach, prevents the user from having to search three separate sections. Filters can allow drilling down: e.g. Type = Capability vs Blueprint, Category = “Monitoring” or “Collaboration”. Cards with icons and concise descriptions would represent each item (Capability cards showing the service logo and what it does, Blueprint cards showing the flow of logos or a brief title, etc.). The goal is to make browsing intuitive and all-encompassing – a user exploring the library should uncover not just tools, but ready-made solutions and agent behaviors.

Concept 2: Blueprint-Centric Discovery with Guided Setup – Many users will come with a use-case in mind (“I want to automate our release notifications” or “help with compliance checks”). Harmony should offer a Blueprint Gallery where workflows are categorized by use case (similar to Zapier’s template use-cases). Each Blueprint listing can display which Capabilities it uses (with icons) and which Skills or AI abilities it involves (e.g. “uses GPT-4 for summarization”). Clicking a Blueprint would show a summary diagram of its steps and an “Install Blueprint” flow: the platform would check which Capabilities are not yet installed/authorized and walk the user through connecting them (for example, if a “Deploy Approval Workflow” blueprint requires GitHub and Slack, Harmony would prompt: “Connect your GitHub account” then “Connect your Slack workspace”). This concept ensures the library not only shows what’s possible but provides a turnkey way to adopt a workflow. It’s a best practice seen in Harness and others – template libraries that instantly bootstrap a user’s environment with the new workflow, including all necessary credentials and configurations.

Concept 3: Relationship Map & Skill Explorer – To encourage discovery of Skills (the atomic instructions or agent behaviors), Harmony can include a specialized view that maps Skills to the larger workflows they’re part of. For instance, a Skill Explorer might list agent Skills (like “Escalate incident to on-call engineer” or “Run diagnostic script”) and show how frequently each is used and in which Blueprints. Selecting a Skill could present a small graph: the Skill at center, connected to Blueprints that use it, and Capabilities it touches (if any). This is analogous to viewing a function’s call graph in code. It helps users understand the context of a Skill (e.g. “This ‘Run AWS Diagnostics’ skill is used in 3 Blueprints: Cloud Outage Response, Cost Anomaly Investigation, Security Incident Triage”). From there, the user might navigate to one of those Blueprints or decide to use that Skill in a new custom workflow. This relational UX fosters reusability – users see that Skills are modular and can be borrowed or modified. Additionally, providing an “Add Skill to Blueprint” action right from this view (perhaps launching a wizard to insert that skill into an existing workflow) would streamline the process of composing new workflows from known parts.

For all the above concepts, search is a critical overlay. A global search bar (with quick results grouped by Capabilities/Blueprints/Skills) can implement Concept 1 on the fly, much like how modern IDEs search across symbols. And visually, maintaining consistency (e.g. a card style or list with clear icons for each item type) will help users distinguish Capabilities (tools/services), Blueprints (end-to-end recipes), and Skills (building blocks) at a glance.

Generative Workflow Patterns (Natural Language to DAGs)

Converting a user’s intent (expressed in natural language) into a reliable workflow is an AI-era design challenge. Two approaches have emerged:

“Direct-to-Code” Generation: In this approach, an LLM takes the user’s prompt (e.g. “Create an incident response workflow that pages Slack, opens a Jira ticket, and runs a diagnostic script if CPU > 80%.”) and directly produces executable code for the workflow. This could mean generating a Temporal TypeScript function or similar code that implements the logic. Tools like Replit’s AI Agent essentially do this for apps – “Tell Replit Agent your app idea... and it will build it for you automatically”, resulting in actual code and UI that the user can run. The benefit is speed – one shot to a working prototype – and the ability to leverage the full expressiveness of code. However, the drawbacks are lack of transparency and editability: the user, especially a non-developer, might find it hard to understand the generated code or spot errors. Any hallucination or logic bug in the code might not be evident until it runs (which in Ops could be dangerous). Essentially, it bypasses a human check, putting a lot of trust in the AI to get it right in one go.

“Intermediate Representation” (IR) with Human-in-the-Loop: Here, the LLM generates a structured outline of the workflow – for example, a JSON/YAML draft or a visual draft – instead of final code. This IR could list the steps, branches, and tools in an abstract form. The user then reviews this draft in a UI (possibly the infinite canvas or a step list) and confirms or adjusts it, after which the system compiles it to code (or otherwise executes it). This pattern is about keeping the human in the loop to verify the AI’s plan. For instance, an LLM might output a JSON like: { "steps": [ {"tool": "Slack", "action": "send message", ...}, {"tool": "Jira", "action": "create issue", ...}, ... ] }. The user would see a visual workflow constructed from that, tweak any mistakes (maybe the AI chose the wrong field or missed a condition), and only then save or run the workflow. Microsoft’s Copilot for Power Automate follows a similar idea: it suggests a flow in plain language steps which the user can modify before implementation. This approach aligns with how the n8n community imagines LLM assistance – “n8n would draft this workflow for you to review, modify, or extend”. It’s safer and more transparent: the user becomes the editor or director, not just the prompter.

Recommendation – Hybrid Flow with Human Confirmation: For Harmony, the IR approach is preferable, possibly combined with immediate execution of parts for fast feedback. A best-of-both-worlds architecture might work like this: when a user describes a desired workflow in chat, the LLM produces a draft Blueprint (in Harmony’s JSON/YAML schema). The UI renders this draft on the canvas for the user to inspect. The user can click each step to see details or edit parameters. Harmony’s agent could even explain each step’s rationale in a sidebar (building user trust by showing it “thought” to include, say, a delay step for cooldown). Once satisfied, the user hits “Compile & Deploy”, and the IR is converted to real Temporal code or executed via Harmony’s engine. This flow ensures the AI is an assistant, not an automaton. It leverages the LLM’s ability to cover boilerplate and suggest best practices (e.g. “Add a retry here”), while leveraging the human’s judgment to verify things (like not sending a page at 3 AM unless necessary).

From an architecture standpoint, Harmony could maintain a library of approved building blocks (for example, known-good Skill implementations or tested API call templates) that the LLM references when composing the IR. That way, when the LLM says “create Jira ticket” in the IR, it’s using a proven template for that action. This further increases reliability of direct code generation, bridging the gap between free-form AI output and strict determinism. As a parallel, consider how OpenAI function calling lets an LLM output a JSON that matches a schema – we’re effectively asking the LLM to output a workflow schema that Harmony defines. The system can validate the draft (checking types, required fields) before it ever runs. This validation step is crucial for human-in-the-loop UX: the user should be alerted to any logical inconsistencies (“Step 3 references an output of Step 2 that doesn’t exist”) before running the workflow.

In summary, the generative workflow UX for Harmony should feel like a dialogue: The user says what they want, the AI proposes a plan, the user and AI iterate (via chat or direct editing) on that plan, and finally the plan materializes into running automation. This keeps the human firmly in control, with the AI doing the heavy lifting under supervision – an ideal balance for high-stakes engineering operations.

AI-Native Information Architecture

Designing Harmony’s information architecture (IA) requires rethinking the classic SaaS layout for an AI-driven, agent-centric context. Traditional platforms often rely on static sidebar navigation (sections like “Dashboards”, “Workflows”, “Settings”). While a sidebar can give users a consistent anchor, it may constrain an interface meant to be fluid and context-driven.

Object-Oriented UI (OOUI) vs. Intent-Oriented UI: An OOUI focuses the IA around key objects in the system – essentially turning nouns of the domain into primary navigation items. For Harmony, the core objects might be Capabilities (integrations), Workflows/Blueprints, Agents (if Harmony has distinct agents), Incidents (as records of workflow runs or events), and Skills. Listing these in a sidebar is straightforward, but AI-native design suggests we might need more nuance. An Intent-Oriented UI centers around what the user wants to accomplish at any given moment. Instead of asking the user to find the right section, the interface presents the tools or views relevant to their current intent. For example, if the user’s intent is “Resolve an ongoing incident,” the UI could morph to show an Incident Command Center view – regardless of whether that data lives under “Incidents” or “Workflows” in a traditional hierarchy. In an agentic system, the agent can help surface the UI needed. Think of it like this: old IA was like a fixed map, but an intent-based IA is like a smart GPS – it brings you to the screens you need based on context and queries.

Global Nav vs. Contextual Views: We recommend a hybrid approach. Maintain a minimalist global navigation for anchoring – perhaps an icon-based sidebar with only a few entries: Home (Overview), Library (the unified marketplace of Capabilities/Blueprints/Skills), Active Operations (current running workflows/incidents), and History (logs, past runs). These cover the major objects in Harmony. But beyond this, rely on contextual navigation within each section that adapts to the user’s journey. For example, if a user enters an Active Incident (workflow run) from the Active Operations list, the sidebar could temporarily expand or transform to show sub-sections relevant to that incident: e.g. Timeline, Agents Chat, Logs, Metrics, Related Docs. Once inside a workflow context, the user shouldn’t have to manually click into other top-level sections – everything needed is either in that view or one click away. This is similar to how modern project management tools work: clicking a project opens a contextual sub-menu just for that project, while the main nav minimizes.

Agent as Navigator: Since Harmony is AI-native, consider the AI agent as a part of the navigation UX. For instance, a chat interface (perhaps always accessible in a corner) can act as a command palette. A user could type “Show me my compliance workflows” or even just “Go to Slack integration settings,” and the UI would jump to the appropriate screen (triggering the relevant component). This is analogous to a user using Spotlight search or a command palette in software – but powered by intent parsing. It creates an Intent-Oriented UI on top of the OOUI foundation: the objects exist, but the agent helps the user find and manipulate them with natural language. This concept is supported by emerging patterns like MCP (Model-Context-Plan) UI in agent frameworks, where user actions are expressed as intents that the agent routes to UI components. In Harmony, an agent could interpret “I want to add a new capability for AWS” as an intent to open the Library filtered to AWS integrations, essentially shortcutting the manual navigation.

Managing Complexity with OOUI: The OOUI concept also implies that Harmony’s design should treat key entities (like an “Incident” or a “Workflow”) as first-class, almost tangible objects in the interface. Each could have a dedicated page or panel with its properties and actions. This is in line with design systems thinking in 2025: as one article notes, focusing on “recurring objects and relationships that drive everything behind the UI” leads to more meaningful interfaces. For example, an Incident page in Harmony might show its status, related runbook, involved systems (as Capabilities), and an activity feed. The relationships between objects (which we also leverage in the Library UX) should reflect in navigation – e.g., from an Incident you might navigate to the underlying Blueprint that generated it, or to the specific Capability (like PagerDuty) that was used to page the on-call. In essence, navigation becomes non-linear and graph-like: users follow the object relationships as much as they use menus. This is very suitable for an AI platform because an AI agent can recommend which related object the user might need to see (“This incident’s root cause analysis is in the related Problem ticket; shall I open it?”).

Rethinking the Sidebar: If we do keep a sidebar, it could be dynamic. Imagine the sidebar has two modes: a primary menu for the major sections, and a contextual menu that appears below it for the current focus. For an agentic feel, the sidebar (or top nav) might even highlight Suggested entries – for example, if an agent determines that a certain compliance workflow is failing, it could highlight the Compliance section or directly a link “⚠️ Compliance Scan (needs attention)”. Traditional IA wouldn’t do that – but an AI-driven IA can be situation-aware.

In conclusion, Harmony’s IA should strive to be adaptive. Use OOUI principles to ensure the design aligns with the real-world entities Ops engineers deal with (tools, workflows, incidents), giving a sense of familiarity and structure. At the same time, layer on intent-based interactions so that users (in collaboration with the AI) can fluidly navigate without being trapped in a rigid menu sequence. By balancing a small, clear global structure with context-sensitive navigation and agent-assisted shortcuts, Harmony’s interface will feel both coherent and contextually intelligent – a true AI-native experience rather than a retrofitted traditional app.

Hybrid Chat-Canvas UX

A hallmark of an AI-native Ops platform will be the interplay between conversational interfaces and visual interfaces. Users should be able to fluidly move from telling an AI what they want, to manually adjusting it on a canvas, and back again. Let’s examine analogous tools and then outline Harmony’s approach:

Replit Ghostwriter & Bolt.new (Conversational Builders): Replit’s AI “Agent” allows a user to literally request an app or feature in chat and have it generated in their development environment. It’s like pair-programming with an AI: you say “I need a web form with a signup,” and the code appears, which you can then run and tweak. Bolt.new applies a similar concept to no-code app building – you chat your way to a prototype. Bolt’s interface, as reported by users, forgoes a complex drag-drop UI in favor of just conversation: “Simply type your idea into the chat... and Bolt will start building”. These tools highlight a key advantage of chat: speed and approachability. Especially for non-engineers, describing a need in natural language is far easier than manipulating a formal UI from scratch.

Cursor & IDE Agents (Chat + Direct Edit): In software like Cursor (an AI-enhanced code editor), there’s a tight loop between chat and manual editing. A developer might accept an AI-suggested code block, then type some changes, then ask the AI in chat to refactor it. Cursor even has a mode to generate a “plan” for code changes that the user can review in markdown before committing. This is essentially a chat-canvas hybrid in a coding context – the code editor is the canvas, and the AI is conversed with on the side. The user can always intervene directly. This constant back-and-forth is critical: it keeps the human in control and the AI in a supportive role.

Proposed Harmony Hybrid UX: Harmony should offer an integrated Chat + Canvas interface for building and managing workflows. Here’s a potential user flow to illustrate the experience:

Conversation to Create: The user starts in a chat pane (perhaps labeled “Harmony Assistant” or similar) and says, “Create a workflow for incident response when a server is down. It should notify on Slack, create a Jira ticket, run a diagnostic script, and await human approval if critical.” The AI might ask a clarification or two (agentic systems often do) and then generate a draft workflow on the Canvas. This means behind the scenes it created nodes for Slack alert, Jira ticket, script execution, an approval step, etc., and laid them out on the canvas with arrows connecting them in sequence or branching as needed.

Visual Review & Editing: The user now switches attention to the canvas, where the workflow is drawn out as a directed graph or flow diagram. They see something like: Trigger: Server Down event → Slack: post message → Jira: open ticket → Parallel: (Run Diagnostics) and (Wait for Approval), etc. Now the user can directly interact with this. Maybe they drag a new node in if they see something missing, or they click on the Slack node and change the message text in a properties panel. The canvas supports typical drag-and-drop editing: rearranging the flow, connecting lines, adjusting conditional branches. The key is that the canvas is fully synchronized with the chat’s understanding of the workflow. Perhaps each node could even have a little “explanation” the AI provided (accessible on hover) to describe what it does, bridging the gap between the AI’s reasoning and the visual element.

Natural Language Revision: Now, suppose the user wants to make a change but prefers not to fiddle with the nodes manually. They can return to the chat and say something like, “Add a condition: only create a Jira ticket if the diagnostics indicate an application issue, otherwise just comment on Slack.” The AI can interpret this and modify the workflow on the canvas accordingly – e.g. insert a decision diamond after the diagnostics node, linking to Jira only on a certain condition. The canvas updates in real-time, highlighting the newly added logic. This is the revision via chat step, showing that even after initial creation, the user can use natural language to adjust the workflow. It’s like having a conversation with a co-worker who is simultaneously drawing a diagram that you both refine together.

Execution and Monitoring via Chat+Canvas: The hybrid model can extend into runtime as well. Imagine the workflow is running (during a real incident). The canvas could show the live state (flashing the node that’s currently active, drawing checkmarks on completed steps – similar to Temporal’s timeline but on the diagram). Meanwhile, the chat can stream live updates or allow queries: “What’s the status now?” and the agent can highlight the current node on the canvas and say, “Awaiting approval from on-call engineer.” Conversely, the user might even approve via chat (“approve the fix now”), which the agent takes as an intent to proceed the workflow. This synergy means the user can either click a button on the canvas (maybe an “Approve” node has a UI button) or just say it in chat – whichever is more convenient. Both drive the same underlying logic.

Balancing Modalities: The design should encourage using the right tool for the task. Chat is excellent for abstract or high-level instructions (“make me a thing that does X”) and quick tweaks (“change X to Y”). The Canvas is excellent for precision, complex conditional logic, and giving an overview of the whole workflow. In Harmony, users might oscillate: start with chat to generate 80% of the workflow, use canvas to fine-tune the tricky 20%, and maybe use chat again to explain or adjust something globally (“apply this to all steps”). A successful hybrid UX will make it feel seamless – e.g. the chat could automatically describe what changed whenever a user drags something on canvas (“You moved the diagnostics step to run in parallel with the Slack notification.”) and conversely, after a chat command, the UI could briefly highlight the affected nodes (“Condition node added here”).

Learning from Others: Tools like Bolt.new for automation claim to avoid “frustration-laden drag and drop” by leaning on AI – we take that as a warning not to force manual editing when the AI could do it faster. At the same time, pure chat can be opaque; the user might not know what the AI really did without a visual. By marrying the two, Harmony ensures the AI’s actions are always visible and editable. Replit’s approach of letting users open the code after generation is analogous – you always have a way to verify and tweak. In Harmony, the canvas (and possibly a togglable JSON/code view for power users) serves as that transparency mechanism.

In conclusion, Harmony’s North Star is a truly bidirectional chat-canvas interface: the chat is not just a gimmick on top of a UI, and the canvas is not just a static diagram – they are two manifestations of the same underlying system state, each one live-updating with changes from the other. This empowers users to engage with the system in the mode they prefer at any moment. An Ops engineer might brainstorm in conversation mode and implement in visual mode, or vice versa. By supporting both, Harmony will accommodate different working styles and make complex automation design as approachable as having a conversation, while as precise as drawing a diagram. This hybrid approach will set Harmony apart in usability, making the creation and management of workflows feel like a collaborative experience between the user and an AI co-engineer.

North Star UX Concepts for Harmony

Conversational Orchestration: “Speak, and it shall be built.” – This concept envisions Harmony as a platform where initiating any operation is as simple as having a conversation. Users describe objectives in natural language, and Harmony’s AI interprets and orchestrates the necessary workflows. The UI for this is a chat-centric home screen, where the user might say “I want to onboard a new engineer” or “Deploy the latest build to staging.” Harmony then generates the workflow or executes the sequence, asking for confirmation or details as needed. The system acts like a DevOps co-pilot, capable of translating requests into actions across tools (CI/CD, incident management, etc.). The key UX goal here is minimal friction: users shouldn’t have to click through menus or forms for common tasks – they can just tell Harmony their intent and see it done (with transparency). This concept fits an agentic Ops platform because it leverages the agent’s understanding to streamline operations. All the underlying complexity (which normally would involve navigating to a pipeline screen, filling out a form, selecting approvers, etc.) is hidden behind a conversational interface. Harmony becomes a kind of command center you talk to. The challenge (and design principle) is to maintain user trust and control – so the conversation is augmented with confirmations (“Shall I proceed to deploy version 1.2.3 to staging?”) and the ability to review the plan (perhaps by Harmony showing a quick outline or canvas of what it’s about to do for approval). This North Star concept ensures Harmony feels less like a dashboard and more like a collaborative AI assistant for Ops.

Unified Operations Workspace: “One screen to rule them all.” – This concept imagines that an Ops engineer shouldn’t have to jump between disparate tools and tabs – Harmony provides a single-pane-of-glass where everything converges. In practice, this means a customizable canvas or dashboard that can embed various contexts: live chat from an incident war room, a timeline of workflow executions, system metrics from monitoring tools, and quick actions for remediation – all in one view. Think of it as the “Ops cockpit.” For example, during an outage, an engineer could open the Harmony workspace and see: an agent summary of the incident (from monitoring alerts), the running incident workflow (with a visual flow of what steps have executed), a chat thread with colleagues and AI recommendations, and widgets showing key metrics (CPU, error rates). Rather than context-switching between Slack, PagerDuty, Jenkins, etc., Harmony’s UX aggregates these via Capabilities and presents them in an integrated way. The design would likely allow modules or cards to be arranged (somewhat like a customizable dashboard, but dynamic). The AI plays a role by highlighting pertinent information (e.g., automatically surfacing a “Root cause hypothesis” card if it detects one). This North Star aims for situational awareness – at any given moment, Harmony’s interface should answer: What’s happening? Where? What’s next? It’s inspired by how tools like Mission Control in space flights or gaming HUDs work – everything essential is in view. For Harmony, achieving this means tight integration of data and actions: you can both monitor and act from the same interface. It’s ambitious, but it aligns with making an AI Ops platform truly feel like the central brain of operations.

Intelligent Contextual UI: “The interface that adapts to you.” – The idea here is that Harmony’s UI is not one-size-fits-all; it dynamically reshapes based on context, driven by the agent’s understanding of the user’s goals (the Intent-Oriented UI we discussed). In practical terms, when a user is designing a workflow, the interface might highlight the Library panel (suggesting relevant Capabilities or Skills) and present an “AI Suggestions” sidebar (with ideas or best practices for that workflow). However, when the user switches to a runtime mode (say, monitoring a deployment), the design shifts: the Library panel might hide, replaced by a Incident Details panel, and the color scheme or alerts become more prominent. This is somewhat like “modes” in an application, but instead of the user explicitly switching modes, Harmony infers it. It’s intent-driven personalization. For example, if Harmony detects the user is frequently working with compliance workflows, the UI might surface a Compliance Hub on the main nav or as a homepage section – showing recent compliance runs, outstanding issues, and a prompt like “Need to create a new policy check?”. Another scenario: if an incident is critical, the UI could enter a “focused mode” where distractions are minimized (collaboration chat is front and center, all configuration options tucked away) to help the user concentrate. This concept basically treats the UI as fluid, with the agent as a real-time UI designer optimizing the layout for the task at hand. We see early hints of this in products that have AI-driven dashboards or adaptive help prompts, but Harmony can take it further. The end result is a system that feels alive and responsive – new users get guided to what they need, power users see what’s relevant without clicking around, and everyone feels the interface is almost “reading their mind” in anticipating needs. It’s the opposite of static navigation: it’s an interface that’s as agile as the agile workflows it manages.

(Bonus) Explainable Autonomy: “No black boxes – every action explained.” – This concept underpins user trust by ensuring that wherever the AI is taking action or making decisions, the UX provides an accessible explanation or audit trail. In an Ops platform, an agent might do things like auto-escalate an incident, roll back a deployment, or skip a workflow step due to some condition. The North Star vision is that Harmony never does this opaquely. The UI would include explainability features such as: hovering on an automated action shows the reason (e.g., “Skipped Step 4 because the server health check passed, as per policy”), or an “Agent Thoughts” log in chat during an autonomous workflow showing what the agent is considering (much like seeing a reasoning trace). While not explicitly requested in the prompt, this concept complements the others by addressing a core requirement in AI Ops – trust and auditability. Visually, this might be implemented as small info icons or a dedicated “Explain” button next to AI-driven actions. Users could click it and see a brief natural language explanation possibly with links to more details or logs. Since Harmony is agent-driven, making the agent’s “thought process” visible (when asked) turns it from a magic box into a collaborative partner. This concept is akin to having a GPS that not only gives you directions but can tell you why it chose a route if you ask – critical in situations where stakes are high and users need reassurance that the AI is acting correctly.

(The above “bonus” concept is included as an extension of the trust theme, which is crucial in an AI-native system handling real operations. It can be considered part of the North Star vision for Harmony’s UX even though it makes our list four items – it’s worth mentioning as it binds all the others with a layer of user confidence.)

Recommended Generative Workflow Architecture

Finally, synthesizing all the research and concepts, here’s a recommended architecture and UX flow for Harmony’s generative workflows, marrying the best practices:

Intermediate Plan Generation: When a user requests a new workflow (via chat or form), Harmony’s LLM agent first generates an intermediate plan – think of this as a pseudo-workflow in JSON or a hidden draft. This plan includes each step, tool, and branching logic in a structured way. Immediately show the user a “Workflow Draft” on the canvas or as an outline. This visual draft is crucial for the human-in-the-loop approach (the user sees what the AI intends to do before it’s live). It’s analogous to how GitHub Copilot might suggest code which you review before committing. Here, the user reviews the workflow draft.

Schema Validation & AI Feedback Loop: Harmony’s engine validates the draft against known schemas and policies. For example, if a step references a Capability the user hasn’t installed, that’s flagged. If a conditional branch has no else, maybe that’s warned. Any validation issues can be fed back to the AI (to regenerate or fix) or presented to the user. The LLM could even be prompted to self-check the plan (“Ensure the workflow JSON meets all required fields and best practices”). By enforcing a schema on the LLM outputs and using functions (like OpenAI function calling), Harmony ensures the AI sticks to a predictable format – this reduces hallucinations and increases reliability of the auto-generated workflow.

User Review & Edit Cycle: The user then tweaks the draft as needed – using either direct manipulation (dragging nodes, editing fields) or asking the agent in chat to adjust. Each change updates the intermediate representation. This phase can loop until the user is satisfied. It’s essentially a design sandbox. Nothing runs for real until explicit approval. This addresses human-in-the-loop: the workflow is generative but not autonomous until the user says go. It’s the equivalent of editing a config file or flowchart that the AI started for you.

Compilation to Executable Code: Once approved, Harmony compiles the final IR into the actual executable form (Temporal TypeScript code, or a series of API calls orchestrated by the platform). Because the IR was assembled from known building blocks (each Skill could map to a code function or API call), the compilation is deterministic. In fact, many parts might not need classic “compiling” – for example, if the IR is a JSON that the runtime engine can interpret directly (like how AWS Step Functions take a JSON state machine and just execute it). The key is, the user doesn’t have to write code, but under the hood Harmony either generates code or uses a code-less orchestration to implement the workflow logic.

Live Run & Agent Oversight: When the workflow runs, the same AI agent that helped build it can also monitor it. If something unexpected happens (e.g., an API call fails), the agent can ask the user via chat, “The deployment step failed due to a timeout. Should I retry or roll back?” – effectively giving a conversational control layer over the running workflow. This ties back to the hybrid chat-canvas UX: the user could intervene in a running workflow by simply telling the agent what to do next (within allowed bounds). The architecture thus isn’t just generate-and-forget; the agent continuously assists during execution. Moreover, data from runs can feed back into future generation – Harmony can learn from previous executions (success or errors) to suggest improvements to workflows (perhaps warning the user next time, “Last time this step timed out – consider adding a retry logic,” which the user can accept and the AI will insert).

Continuous Improvement & Versioning: Every generated workflow is stored with version history. If the AI suggests an update or the user modifies it later, Harmony can show a diff or changelog (to maintain transparency of what the AI changed versus what a human changed). This allows rollback if a generative edit goes wrong. It also means Harmony can eventually optimize – e.g. using analytics to find if certain manual edits are always done after generation (indicating the AI’s initial pattern could be updated). For example, if users often add a “Notify team” step at the end of incident workflows, Harmony might start including that in the first draft proactively.

Strategy Summary: The generative workflow architecture for Harmony is centered on safety, transparency, and collaboration. Unlike a black-box auto-coder that spits out a script, Harmony’s approach is to have the AI and human build the workflow together. By using an intermediate visual/editable representation, Harmony ensures the user understands and approves the logic. By validating and using known integrations, it avoids common pitfalls of DSLs and graphs that previous tools struggled with (where parts of logic were hidden in code snippets, causing maintenance headaches). In essence, we borrow the reliability of workflow-as-code (strict, versionable, testable) while delivering it through a no-code UX. That is the technical magic Harmony aims to achieve, and with the UX guidelines above, the user will experience it as an intuitive, powerful system – the AI does the heavy lifting, but the user remains the architect of their workflows.